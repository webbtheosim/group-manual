<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Computing Resources &mdash; Webb Group Manual  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="RELEVANT READING" href="../READING/content.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../toc.html" class="icon icon-home"> Webb Group Manual
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">OVERVIEW</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GROUP/content.html">THE GROUP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../READING/content.html">RELEVANT READING</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Computing Resources</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#local-workstation">Local Workstation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#princeton-research-computing">Princeton Research Computing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#onboarding">Onboarding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#computing-systems">Computing Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="#file-systems-storage">File Systems/Storage</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#external-computing">External Computing</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../toc.html">Webb Group Manual</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../toc.html" class="icon icon-home"></a> &raquo;</li>
      <li>Computing Resources</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/webbtheosim/group-manual/blob/main/site/source/COMPUTING/content.ipynb" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="computing-resources">
<h1>Computing Resources<a class="headerlink" href="#computing-resources" title="Permalink to this heading"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading"></a></h2>
<p>As a computational group, office and computing resources are naturally very important for us. Therefore, it is important to me that you <em>(i)</em> have a comfortable, ergonomic working environment and <em>(ii)</em> have adequate resources for your project.
Both are essential to conducting research effectively and efficiently. If either are not true, then please do not hesitate to discuss with me.</p>
</section>
<section id="local-workstation">
<h2>Local Workstation<a class="headerlink" href="#local-workstation" title="Permalink to this heading"></a></h2>
<p>If you are a graduate student or postdoctoral researcher, we will dicuss a preferred local computing setup for work in the office. Depending on what the researcher wants to do, this amounts to purchasing a new local workstation, laptop, or set of peripherals (e.g., monitors, keyboards, trackpads).
In addition to using the workstation for all standard personal computing needs, the workstations are used for interfacing with other computational resources and for running/testing small calculations. We do not expect these machines to be able to handle large simulations, but you might find them useful for analysis, preparing graphics, training ML models, coding, etc. We may also maintain group machines that will be dedicated to running software that have single-machine licenses, churning out small calculations, or rendering graphics.Your local computer must be reasonable enough to run standard software, perform code testing, and perhaps some analysis/machine learning. I strongly recommend that your local work computer (whether it be a laptop + external monitor or a desktop) be a Mac or run Linux.</p>
<p>There are a variety of existing workstations/external monitors that may be suitable for use by grad students/postdocs/undergrads as desired and available. If you think you need something, let me know.</p>
</section>
<section id="princeton-research-computing">
<h2>Princeton Research Computing<a class="headerlink" href="#princeton-research-computing" title="Permalink to this heading"></a></h2>
<p>Our research is principally performed on systems maintained by <a class="reference external" href="https://researchcomputing.princeton.edu/">Princeton Research Computing</a> (PRC). PRC follows a community resource model. This means our group does not “own” a specific cluster or even a fraction of a specific cluster. Instead, systems are acquired through aggregate purchases by several groups or organizations on campus. Because many partiues participate in the acquisition, the machines are usually larger, and this manifests in better pricing. In addition, the university itself is also very supportive of computational research and will often cost-share in the acquisition of new, state-of-the-art resources, should the community demonstrate sufficient need. Consequently, virtually anyone at Princeton can obtain an account and utilize the computing systems; however, groups that finanacially contribute to an acquisition receive <a class="reference external" href="https://researchcomputing.princeton.edu/support/knowledge-base/job-priority">priority</a> for their calculations via a fairshare system. In any case, the computing resources and staff support at Princeton are excellent and have so far met our computational needs. We will routinely contribute to new systems when the opportunities arise.</p>
<section id="onboarding">
<h3>Onboarding<a class="headerlink" href="#onboarding" title="Permalink to this heading"></a></h3>
<p>When you join the group, Prof. Webb will first request your access to relevant computing and file systems for the group. Once you have an account, you can proceed with the following tasks. Your specific objectives are to <em>(i)</em> capably log into and navigate an intended machine/cluster, <em>(ii)</em> understand when and how to properly utilize the Slurm scheduler, and <em>(iii)</em> abide by and understand cluster etiquette.</p>
<ol class="arabic simple">
<li><p>Read the <a class="reference external" href="https://researchcomputing.princeton.edu/get-started/guide-princeton-clusters">Guide to Princeton’s Research Computing Clusters</a> and work through the exercises.</p></li>
<li><p>Read the <a class="reference external" href="https://researchcomputing.princeton.edu/get-started/mistakes-avoid">Mistakes to Avoid</a>. Then, generally avoid those mistakes. There are <em>some</em> exceptions, which we will try to touch on elsewhere, but by in large, follow the rules.</p></li>
<li><p>Make note of various <a class="reference external" href="https://researchcomputing.princeton.edu/learn/workshops-live-training">Workshops</a> and either attend such workshops or review the materials from any <a class="reference external" href="https://researchcomputing.princeton.edu/learn/workshops-live-training/archives-past-workshops">archived sessions</a></p></li>
<li><p>Review our own group video Tutorials on cluster usage.</p></li>
</ol>
</section>
<section id="computing-systems">
<h3>Computing Systems<a class="headerlink" href="#computing-systems" title="Permalink to this heading"></a></h3>
<p>We predominantly use the following systems:</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://researchcomputing.princeton.edu/systems/stellar">Stellar</a></strong>: <em>296 Intel Quad Cascade Lake nodes (96-cores/node); 6 GPU Nodes (AMD Rome + 2 NVIDIDA A100 GPUs / node); 187 AMD Rome nodes (128-cores/node)</em> - this is our major workhorse cluster. The cluster is relatively exclusive by comparison to some of the other Princeton clusters, and we have good priority access based on financial contributions. The size of nodes are well-suited for the majority of our calculations.</p></li>
<li><p><strong><a class="reference external" href="https://researchcomputing.princeton.edu/systems/tiger">TigerCPU</a></strong>: <em>408 Intel Skylake nodes (40-cores/node)</em> - this is an older machine/architecture compared to Stellar. In case you are running into job limits on Stellar, it may be reasonable to consider sending some jobs here. However, TigerCPU has an overall larger user base than Stellar, which can mean that queue times may be longer.</p></li>
<li><p><strong><a class="reference external" href="https://researchcomputing.princeton.edu/systems/tiger">TigerGPU</a></strong>: <em>80 Intel Broadwell nodes (28-cores/node + 4 NVIDIA P100 GPUs/node)</em> - Technically, this is not a distinct cluster from TigerCPU; they both comprise Tiger. On the other hand, this portion has different Intel nodes and GPUs. A lot of our calculations/software can use GPUs pretty effectively. TigerGPU itself is often underutilized, and so if your application can use GPUs, you should definitely consider sending some jobs here.</p></li>
<li><p><strong><a class="reference external" href="https://researchcomputing.princeton.edu/systems/della">Della</a></strong>: This is a highly heterogeneous cluster. It is not commonly used in the group, but it should be on your radar.</p></li>
</ul>
<p><strong>Note:</strong> One thing to be aware of is that all of the machines, except for Della, are moreso intended for large-scale parallel jobs rather than (many) small/serial jobs. Yet, we often have the need for the latter. To do so, we play games with job packaging/bundling/deployment to work within the constraints provided by the scheduler and the overall specifications/etiquette on the cluster.</p>
<p><strong>Note:</strong> If you are ever uncertain about whether doing something on the cluster is OK, and you cannot easily find the answer, <em><strong>ask</strong></em> me or someone else in the group first.</p>
</section>
<section id="file-systems-storage">
<h3>File Systems/Storage<a class="headerlink" href="#file-systems-storage" title="Permalink to this heading"></a></h3>
<p>To use the clusters most effectively, it is advantageous to understand the purpose of the various filesystems. Please read the overview  <a class="reference external" href="https://researchcomputing.princeton.edu/support/knowledge-base/data-storage">here</a>. We have a lot of storage where it is important. If we are running into storage issues, please bring it up at a group meeting so that we can discuss. If there is an actual good reason to have as much data, then we can request a quota increase, and PRC is generous with us in this regard. However, I do not want to request increases just because we are generating data unnecessarily.</p>
<ul class="simple">
<li><p>/home/ - this is pretty much just for your compiled software executables, analysis scripts, and other lightweight stuff (shell scripts, etc.)</p></li>
<li><p>/projects/WEBB - you will be given access to this directory when you join the group. This is predominantly for long-term storage of data and metadata or files that do not change over time. Files can be shared amongst group members here. We have access to 10’s of TB of space, but you should be mindful of what you are putting here. It is not good practice to run any job with heavy I/O from /projects/ since that file system is decoupled from the clusters. For this, you should prefer running your jobs from /scratch/gpfs and moving the results as necessary.</p></li>
<li><p>/scratch/gpfs/<YourNetID> - each cluster is equipped with a proximate, parallel filesystem. PRC really wants you to only run jobs and output results to /scratch. The caveat is that /scratch is <em>not</em> backed up. It is not necessarily cleaned out (this happens on some other clusters I have worked on), but it is not backed up.</p></li>
<li><p>/tmp - if your job for some reason needs really fast I/O, then /tmp accesses a local scratch space on each compute node. This might be the case if you are writing two-electron integral files during an electronic structure calculation. It is pretty rare that we need this, but it exists.</p></li>
</ul>
<p><strong>Other storage available</strong>:</p>
<ul class="simple">
<li><p>physical external hard drives - for archival purposes, we can move data to some of these if needed.</p></li>
<li><p>Dropbox - check out the discussion of our Dropbox folder</p></li>
</ul>
<p><em>Tip</em>: I do not like to clutter scratch. Also, if you are making use of multiple clusters for the same project, then the /scratch directories are different and specific to each. What I will do then is handle everything related to /scratch in the Slurm batch script; an example is shown below. This may not be a good idea, however, if the files that you are working with are very big and would take significant time to transfer.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --job-name=my_job        # create a short name for your job</span>
<span class="c1">#SBATCH --nodes=1                # node count</span>
<span class="c1">#SBATCH --ntasks-per-node=16     # total number of tasks across all nodes</span>
<span class="c1">#SBATCH --cpus-per-task=1        # cpu-cores per task (&gt;1 if multi-threaded tasks)</span>
<span class="c1">#SBATCH --mem-per-cpu=4G         # memory per cpu-core (4G is default)</span>
<span class="c1">#SBATCH --time=00:29:00          # total run time limit (HH:MM:SS)</span>
<span class="c1">#SBATCH --output=output</span>
<span class="c1">#SBATCH --error=error</span>

<span class="c1"># LOAD NECESSARY MODULES/ENVIRONMENTS</span>
module load intel/19.1/64/19.1.1.217
module load intel-mpi/intel/2019.7/64
module load anaconda
conda activate chem-env

<span class="c1"># SET UP JOB</span>
<span class="nv">runDIR</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$SLURM_SUBMIT_DIR</span><span class="s2">&quot;</span>
<span class="nb">echo</span> <span class="s2">&quot;Submitting from </span><span class="nv">$runDIR</span><span class="s2">&quot;</span>

<span class="c1"># MOVE TO SCRATCH</span>
<span class="nb">cd</span> /scratch/gpfs/mawebb
mkdir <span class="si">${</span><span class="nv">SLURM_JOB_ID</span><span class="si">}</span>
<span class="nb">cd</span> <span class="si">${</span><span class="nv">SLURM_JOB_ID</span><span class="si">}</span>

<span class="c1"># MOVE FILES TO SCRATCH</span>
mv <span class="nv">$runDIR</span>/my_inputs* .

<span class="c1"># RUN JOB</span>
&lt;some_executable&gt; ... 

mv ./* <span class="nv">$runDIR</span>/.
<span class="nb">cd</span> ..
rm -r <span class="si">${</span><span class="nv">SLURM_JOB_ID</span><span class="si">}</span>
</pre></div>
</div>
</section>
</section>
<section id="external-computing">
<h2>External Computing<a class="headerlink" href="#external-computing" title="Permalink to this heading"></a></h2>
<p>If we expect or you empirically find that your research will be constrained or limited by access to PRC only, then we should discuss applying for computing time elsewhere. The following is a list of possible resources:</p>
<ul class="simple">
<li><p>Microsoft Azure Cloud Computing - We have dabbled with this. If you are interested, there are frequently opportunities to secure $10k+ in credits relatively easily.</p></li>
<li><p><a class="reference external" href="https://www.alcf.anl.gov/">Argonne Leadership Computing Facility (ALCF)</a></p></li>
<li><p><a class="reference external" href="https://access-ci.org/">Advanced Cyberinfrastructure Coordination Ecosystem: Services &amp; Support</a></p></li>
<li><p><a class="reference external" href="https://www.nersc.gov/">National Energy Research Scientific Computing Center</a></p></li>
<li><p><a class="reference external" href="https://www.olcf.ornl.gov/">Oak Ridge Leadership Computing Facility</a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../READING/content.html" class="btn btn-neutral float-left" title="RELEVANT READING" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Michael A. Webb.
      <span class="lastupdated">Last updated on True.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>